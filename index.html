<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="From Language To Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance">
  <meta property="og:title" content="From Language To Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance"/>
  <meta property="og:description" content="From Language To Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidanceg"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/teaser.png" />
  <meta property="og:image:width" content="1808"/>
  <meta property="og:image:height" content="1014"/>


  <meta name="twitter:title" content="From Language To Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance">
  <meta name="twitter:description" content="From Language To Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/framework.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Motion Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RoboGhost</title>
  
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JHTFKVMFDL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JHTFKVMFDL');
</script>
  
  <link rel="icon" type="image/x-icon" href="static/images/framework.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Language To Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance</h1>
            <h2 class="title is-3 publication-title" style="color:#6e6e6e;"> ICLR 2025</h4>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Zhe Li<sup>1</sup>*,
              </span>
              <span class="author-block">
                Cheng Chi<sup>2</sup>*,
              </span>
              <span class="author-block">
                Yangyang Wei<sup>3</sup>,
              </span>
              <span class="author-block">
                Boan Zhu<sup>4</sup>,
              </span>
              <span class="author-block">
                Yibo Peng<sup>2</sup>,
              </span>
              <span class="author-block">
                Tao Huang<sup>5</sup>,
              </span>
              <span class="author-block">
                Pengwei Wang<sup>2</sup>,
              </span>
              <span class="author-block">
                Zhongyuan Wang<sup>2</sup>,
              </span>
              <span class="author-block">
                Shanghang Zhang<sup>2,6</sup>†
              </span>
              <span class="author-block">
                Chang Xu<sup>1</sup>†
              </span>
              
              
            </div>

            <div class="is-size-5 publication-authors">
              <span class="eql-cntrb"><large><br><sup>1</sup>University of Sydney</large></span>
              <span class="eql-cntrb"><large><br><sup>2</sup>BAAI</large></span> <br>
              <span class="eql-cntrb"><large><br><sup>3</sup>Harbin Institute of Technology</large></span> <br>
              <span class="eql-cntrb"><large><br><sup>4</sup>Hong Kong University of Science and Technology</large></span> <br>
              <span class="eql-cntrb"><large><br><sup>5</sup>Shanghai Jiao Tong University</large></span> <br>
              <span class="eql-cntrb"><large><br><sup>6</sup>Peking University</large></span> <br>
              
            </div>
            <div class="is-size-5 publication-authors">
              <span class="eql-cntrb">* Equal Contribution &nbsp &nbsp † Corresponding Author </span>
            </div>


          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <img src="static/images/framework.pdf" alt="MY ALT TEXT"/> -->
          <p>
            Language plays a vital role in the realm of human motion.
Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and
then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. There-
fore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and
supporting fast, reactive control. A hybrid causal transformer–diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior.
Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework
naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision–language–action humanoid systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="section hero">
  <div class="container is-max-desktop">
      <h2 class="title is-3" align="center">Methodology</h2>
      <div class="content has-text-justified">
        <img src="static/images/lamp.jpg" width="100%" height="100%" />
        <p>
          LaMP overview. We conduct joint training for contrastive learning, matching, and bidirectional text-motion translation by leveraging the textual features extracted from tokenized text descriptions via the text transformer and the motion features derived from the motion transformer.
        </p>
      </div>
      <br>
      <!-- <h2 class="title is-5">LaMP-T2M and LaMP-M2T frameworks overview:</h2> -->
      <div class="content has-text-justified">
        <img src="static/images/t2m.jpg" width="100%" height="100%" />
        <p>
          LaMP-T2M and LaMP-M2T frameworks overview. (Left) Pretrained LaMP’s text transformer is employed to extract condition embedding and autoregressive mask prediction is performed. (Right) Finetuning an LLM to achieve motion captioning.
        </p>
      </div>
  </div>
</section>
<!-- End Method -->


<!-- Experiments -->
<section class="section hero">
  <div class="container is-max-desktop">
      <h2 class="title is-3" align="center">Experiments</h2>
      <div class="content has-text-centered">
        <video id="replay-video"
               controls
               muted
               preload
               playsinline
               width="100%">
          <source src="./static/images/video.mp4"
                  type="video/mp4">
        </video>
        <!-- <a href="https://www.youtube.com/watch?v=HFUe9y-uH-Q" target="_blank"><u>Youtube</u></a> -->
      </div>
      <h2 class="title is-5">Quantitative Results:</h2>
      <div style="text-align: center;">
        <img src="static/images/exp1.jpg" width="80%" height="100%"/>
      </div>
      <br>
      <div style="text-align: center;">
        <img src="static/images/exp2.jpg" width="80%" height="100%"/>
      </div>
      <br>
      <h2 class="title is-5">Qualitative Results:</h2>
      <div style="text-align: center;">
        <img src="static/images/viz2.jpg" width="80%" height="100%" />
      </div>
      <div style="text-align: center;">
        <img src="static/images/viz1.jpg" width="80%" height="100%" />
      </div>
      <div style="text-align: center;">
        <img src="static/images/viz3.jpg" width="80%" height="100%" />
      </div>
  </div>
</section>
<!-- End Method -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{li2025lamp,
    title={LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning},
    author={Zhe Li and Weihao Yuan and Yisheng HE and Lingteng Qiu and Shenhao Zhu and Xiaodong Gu and Weichao Shen and and Yuan Dong and Zilong Dong and Laurence T. Yang},
    journal = {International Conference on Learning Representations (ICLR)}
    year={2025},
}
    </code></pre>
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
