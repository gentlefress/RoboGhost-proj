<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning">
  <meta property="og:title" content="LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioningn"/>
  <meta property="og:description" content="LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/teaser.pdf" />
  <meta property="og:image:width" content="1808"/>
  <meta property="og:image:height" content="1014"/>


  <meta name="twitter:title" content="LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning">
  <meta name="twitter:description" content="LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/framework.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Motion Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LaMP</title>
  
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JHTFKVMFDL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JHTFKVMFDL');
</script>
  
  <link rel="icon" type="image/x-icon" href="static/images/framework.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/aigc3d">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://aigc3d.github.io/mogents/">
            MoGenTS (NeurIPS 2024)
          </a>
          <a class="navbar-item" href="https://aigc3d.github.io/projects/LAM/">
            LAM
          </a>
        </div>
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</h1>
            <h2 class="title is-3 publication-title" style="color:#6e6e6e;"> ICLR 2025</h4>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Zhe Li<sup>1,2</sup>*,
              </span>
              <span class="author-block">
                  <a href="https://weihao-yuan.com/" target="_blank">Weihao Yuan</a><sup>2</sup>*,
              </span>
              <span class="author-block">
                <a href="https://hyshkust.github.io/" target="_blank">Yisheng He</a><sup>2</sup>,
              </span>
              <span class="author-block">
                Lingteng Qiu<sup>2</sup>,
              </span>
              <span class="author-block">
                Shenhao Zhu<sup>2,3</sup>,
              </span>
              <span class="author-block">
                Xiaodong Gu<sup>2</sup>,
              </span>
              <span class="author-block">
                Weichao Shen<sup>2</sup>,
              </span>
              <span class="author-block">
                Yuan Dong<sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=GHOQKCwAAAAJ&hl=zh-CN&oi=ao" target="_blank">Zilong Dong</a><sup>2</sup>†,
              </span>
              <span class="author-block">
                Laurence T. Yang<sup>1</sup>†
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="eql-cntrb"><large><br><sup>1</sup>Huazhong University of Science and Technology</large></span>
              <span class="eql-cntrb"><large><br><sup>2</sup>Alibaba Group</large></span> <br>
              <span class="eql-cntrb"><large><sup>3</sup>Nanjing University</large></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="eql-cntrb">* Equal Contribution &nbsp &nbsp † Corresponding Author </span>
            </div>

            <div class="column has-text-centered">
              <!-- <div class="publication-links"> NeurIPS 2024 </div> -->

              <!-- Arxiv link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.07093" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>

            <!-- Video link -->
            <!-- <span class="link-block">
              <a href="https://www.youtube.com/watch?v=4hMFBvMFSI8" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-youtube"></i>
                </span>
                <span>Youtube</span>
              </a>
            </span> -->

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/gentlefress/LaMP" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code<span>
              </a>
            </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <img src="static/images/framework.pdf" alt="MY ALT TEXT"/> -->
          <p>
            Language plays a vital role in the realm of human motion.
Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs.
This work introduces \textbf{LaMP}, a novel \textbf{La}nguage-\textbf{M}otion \textbf{P}retraining model, which transitions from a language-vision to a more suitable language-motion latent space. 
It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. 
With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning.
For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers.
For retrieval, motion features from LaMP’s motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa.
For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model.
In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. 
Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="section hero">
  <div class="container is-max-desktop">
      <h2 class="title is-3" align="center">Methodology</h2>
      <div class="content has-text-justified">
        <img src="static/images/lamp.jpg" width="100%" height="100%" />
        <p>
          LaMP overview. We conduct joint training for contrastive learning, matching, and bidirectional text-motion translation by leveraging the textual features extracted from tokenized text descriptions via the text transformer and the motion features derived from the motion transformer.
        </p>
      </div>
      <br>
      <!-- <h2 class="title is-5">LaMP-T2M and LaMP-M2T frameworks overview:</h2> -->
      <div class="content has-text-justified">
        <img src="static/images/t2m.jpg" width="100%" height="100%" />
        <p>
          LaMP-T2M and LaMP-M2T frameworks overview. (Left) Pretrained LaMP’s text transformer is employed to extract condition embedding and autoregressive mask prediction is performed. (Right) Finetuning an LLM to achieve motion captioning.
        </p>
      </div>
  </div>
</section>
<!-- End Method -->


<!-- Experiments -->
<section class="section hero">
  <div class="container is-max-desktop">
      <h2 class="title is-3" align="center">Experiments</h2>
      <div class="content has-text-centered">
        <video id="replay-video"
               controls
               muted
               preload
               playsinline
               width="100%">
          <source src="./static/images/video.mp4"
                  type="video/mp4">
        </video>
        <!-- <a href="https://www.youtube.com/watch?v=HFUe9y-uH-Q" target="_blank"><u>Youtube</u></a> -->
      </div>
      <h2 class="title is-5">Quantitative Results:</h2>
      <div style="text-align: center;">
        <img src="static/images/exp1.jpg" width="80%" height="100%"/>
      </div>
      <br>
      <div style="text-align: center;">
        <img src="static/images/exp2.jpg" width="80%" height="100%"/>
      </div>
      <br>
      <h2 class="title is-5">Qualitative Results:</h2>
      <div style="text-align: center;">
        <img src="static/images/viz2.jpg" width="80%" height="100%" />
      </div>
      <div style="text-align: center;">
        <img src="static/images/viz1.jpg" width="80%" height="100%" />
      </div>
      <div style="text-align: center;">
        <img src="static/images/viz3.jpg" width="80%" height="100%" />
      </div>
  </div>
</section>
<!-- End Method -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{li2025lamp,
    title={LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning},
    author={Zhe Li and Weihao Yuan and Yisheng HE and Lingteng Qiu and Shenhao Zhu and Xiaodong Gu and Weichao Shen and and Yuan Dong and Zilong Dong and Laurence T. Yang},
    journal = {International Conference on Learning Representations (ICLR)}
    year={2025},
}
    </code></pre>
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
